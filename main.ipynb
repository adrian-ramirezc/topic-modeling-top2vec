{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# More info: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "# 18K newsgroups posts\n",
    "# 20 topics (train/test) - split based upon date of posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
    "docs = data[\"data\"] # list\n",
    "targets = data[\"target\"]\n",
    "target_names = data[\"target_names\"]\n",
    "topics = [data[\"target_names\"][i] for i in data[\"target\"]] # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document\n",
      "For Sale:\n",
      "\n",
      "OS/2 2.0 Extended Services -\n",
      "\n",
      "        * Extended Database support\n",
      "        * Extended Networking Support\n",
      "        * Remote Host support\n",
      "        * Extended Communication Support\n",
      "\n",
      "PLUS! A copy of OS/2 2.0.  The ES package is brand new and uninstalled, all\n",
      "manuals, disks, etc. are included.  The ES package retails for $495 with OS/2\n",
      "2.0 selling for $79 or something like that.\n",
      "\n",
      "I'll let both of them go for $200.  My needs changed thus eliminating my\n",
      "need for the package once I bought it.\n",
      "\n",
      "If Interested, please Email me at:\n",
      "\n",
      "Topic\n",
      "misc.forsale\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "i = randint(0,18846)\n",
    "print('Document')\n",
    "print(docs[i])\n",
    "print('\\nTopic')\n",
    "print(topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame(data=zip(docs,topics), columns=[\"Document\",\"Topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13518</th>\n",
       "      <td>\\nHaving lived, played, and worked on and near...</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>I am trying to get my system to work with a Ta...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>I just bought a little gizmo that is supposed ...</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>STOP! STOP! STOP! STOP! This argument is getti...</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>\\n\\n\\t\\n\\n\\tHey, Bosio threw a no-no what the ...</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Document  \\\n",
       "13518  \\nHaving lived, played, and worked on and near...   \n",
       "2136   I am trying to get my system to work with a Ta...   \n",
       "5556   I just bought a little gizmo that is supposed ...   \n",
       "9589   STOP! STOP! STOP! STOP! This argument is getti...   \n",
       "2776   \\n\\n\\t\\n\\n\\tHey, Bosio threw a no-no what the ...   \n",
       "\n",
       "                          Topic  \n",
       "13518        talk.politics.guns  \n",
       "2136   comp.sys.ibm.pc.hardware  \n",
       "5556            sci.electronics  \n",
       "9589              comp.graphics  \n",
       "2776         rec.sport.baseball  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/dev/personal/venv/nlp-top2vec/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 23:36:47,238 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-14 23:36:52,471 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-14 23:40:34,033 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-14 23:40:40,399 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-14 23:40:41,264 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(documents = docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of docs per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1501 1100  885  728  654  602  560  394  378  376  372  356  335  309\n",
      "  308  291  287  269  266  244  238  231  229  218  215  214  214  208\n",
      "  198  189  182  176  176  171  167  158  154  142  140  139  125  118\n",
      "  114  114  112  111  108  106  105  104  104  100  100   99   96   92\n",
      "   90   86   83   80   78   77   76   76   75   73   71   71   70   69\n",
      "   69   67   67   67   65   64   64   64   64   62   59   58   57   56\n",
      "   56   56   55   54   53   53   52   52   51   51   51   50   49   49\n",
      "   49   49   48   47   45   45   44   40   39   34   34]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model idetifies 112 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "print(topic_nums[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "words: ['bike' 'car' 'ride' 'bikes' 'honda' 'cars' 'tires' 'brakes' 'mph'\n",
      " 'engine' 'rear' 'wheel' 'gear' 'riding' 'wheels' 'motorcycle' 'brake'\n",
      " 'suspension' 'throttle' 'passenger' 'tire' 'miles' 'toyota' 'ford'\n",
      " 'driving' 'front' 'steering' 'seat' 'bmw' 'shaft' 'valve' 'torque'\n",
      " 'cylinder' 'parking' 'clutch' 'dealer' 'pedal' 'rpm' 'stock' 'bought'\n",
      " 'motorcycles' 'abs' 'highway' 'mileage' 'cage' 'fun' 'exhaust' 'owner'\n",
      " 'buy' 'expensive']\n",
      "1\n",
      "words: ['really' 'think' 'don' 'guess' 'know' 're' 'wouldn' 'you' 'maybe' 'ever'\n",
      " 'just' 'me' 'anything' 'like' 'say' 'why' 'people' 'do' 'something'\n",
      " 'stupid' 'flame' 'someone' 'everyone' 'believe' 'going' 'oh' 'isn' 'tell'\n",
      " 'admit' 'didn' 'feel' 'lot' 'yeah' 'shit' 'things' 'wrong' 'sorry'\n",
      " 'nothing' 'again' 'what' 'even' 'here' 'exactly' 'too' 'thought' 'happen'\n",
      " 'take' 'we' 'about' 'being']\n",
      "2\n",
      "words: ['pitching' 'hitter' 'pitchers' 'pitcher' 'rbi' 'inning' 'innings' 'pitch'\n",
      " 'hitting' 'braves' 'batting' 'pitches' 'pitched' 'hit' 'team' 'teams'\n",
      " 'score' 'season' 'dodgers' 'players' 'career' 'batter' 'baseball' 'game'\n",
      " 'yankees' 'clemens' 'phillies' 'jays' 'sox' 'scored' 'offense' 'walks'\n",
      " 'ball' 'morris' 'reds' 'mets' 'rookie' 'league' 'cubs' 'year' 'alomar'\n",
      " 'bonds' 'gant' 'player' 'defensive' 'winning' 'seasons' 'hits' 'twins'\n",
      " 'era']\n",
      "3\n",
      "words: ['clipper' 'encryption' 'escrow' 'keys' 'nsa' 'secure' 'key' 'encrypted'\n",
      " 'algorithm' 'agencies' 'scheme' 'wiretap' 'crypto' 'enforcement' 'chip'\n",
      " 'decrypt' 'encrypt' 'security' 'government' 'des' 'phones' 'cryptography'\n",
      " 'rsa' 'classified' 'cryptographic' 'denning' 'communications' 'proposal'\n",
      " 'cryptosystem' 'privacy' 'plaintext' 'conversations' 'trust' 'private'\n",
      " 'secret' 'cipher' 'wiretaps' 'tap' 'legal' 'ciphertext' 'court'\n",
      " 'authentication' 'cellular' 'public' 'agency' 'feds' 'chips' 'traffic'\n",
      " 'federal' 'warrant']\n",
      "4\n",
      "words: ['patients' 'treatment' 'symptoms' 'disease' 'patient' 'diet' 'doctor'\n",
      " 'clinical' 'vitamin' 'therapy' 'physician' 'diagnosed' 'yeast'\n",
      " 'treatments' 'infection' 'diagnosis' 'syndrome' 'nutrition' 'chronic'\n",
      " 'medical' 'effects' 'infections' 'doctors' 'candida' 'food' 'dose'\n",
      " 'placebo' 'physicians' 'foods' 'liver' 'eating' 'medicine' 'illness'\n",
      " 'severe' 'pain' 'cancer' 'bacteria' 'cure' 'studies' 'surgery' 'study'\n",
      " 'drugs' 'oral' 'toxic' 'med' 'depression' 'kidney' 'brain' 'breast'\n",
      " 'cause']\n",
      "5\n",
      "words: ['orbit' 'spacecraft' 'launch' 'solar' 'lunar' 'shuttle' 'orbiter'\n",
      " 'orbital' 'satellites' 'moon' 'mission' 'space' 'earth' 'orbiting' 'mars'\n",
      " 'vehicle' 'payload' 'altitude' 'launches' 'missions' 'satellite' 'rocket'\n",
      " 'planet' 'flyby' 'atmosphere' 'telescope' 'jupiter' 'flight' 'surface'\n",
      " 'venus' 'galileo' 'nasa' 'probes' 'kilometers' 'probe' 'launched' 'comet'\n",
      " 'saturn' 'delta' 'project' 'sky' 'planetary' 'propulsion' 'funding'\n",
      " 'exploration' 'titan' 'thermal' 'pluto' 'rockets' 'craft']\n",
      "6\n",
      "words: ['fbi' 'batf' 'waco' 'koresh' 'compound' 'davidians' 'atf' 'fire' 'raid'\n",
      " 'agents' 'suicide' 'grenades' 'assault' 'warrant' 'tear' 'bd' 'knock'\n",
      " 'cult' 'reno' 'warrants' 'tanks' 'happened' 'armored' 'fired' 'gas'\n",
      " 'armed' 'followers' 'they' 'weapons' 'attorney' 'themselves' 'branch'\n",
      " 'children' 'clinton' 'davidian' 'federal' 'guns' 'feds' 'were' 'paranoid'\n",
      " 'arrest' 'burning' 'did' 'tank' 'deaths' 'officers' 'incident'\n",
      " 'investigation' 'police' 'killed']\n",
      "7\n",
      "words: ['god' 'christ' 'jesus' 'lord' 'heaven' 'sins' 'luke' 'faith' 'unto'\n",
      " 'isaiah' 'matthew' 'scriptures' 'psalm' 'holy' 'salvation' 'eternal'\n",
      " 'gospel' 'scripture' 'spirit' 'prophets' 'psalms' 'sin' 'bible' 'romans'\n",
      " 'corinthians' 'moses' 'himself' 'thou' 'messiah' 'father' 'his'\n",
      " 'resurrection' 'disciples' 'verse' 'christians' 'believers' 'thy' 'glory'\n",
      " 'apostles' 'verses' 'church' 'satan' 'prophecy' 'thee' 'revelation'\n",
      " 'prophet' 'him' 'life' 'testament' 'passage']\n",
      "8\n",
      "words: ['belief' 'atheists' 'atheist' 'theists' 'beliefs' 'christianity' 'faith'\n",
      " 'god' 'existence' 'atheism' 'christians' 'religion' 'religions' 'believe'\n",
      " 'truth' 'exists' 'evidence' 'argument' 'christian' 'religious'\n",
      " 'believers' 'bible' 'believing' 'exist' 'arguments' 'gods' 'theism'\n",
      " 'claim' 'accept' 'contradict' 'reason' 'conclusion' 'argue' 'true'\n",
      " 'arrogant' 'reasoning' 'lack' 'contradictory' 'arrogance' 'proof'\n",
      " 'assertion' 'sense' 'therefore' 'fallacy' 'agree' 'meaning' 'absolute'\n",
      " 'definition' 'necessarily' 'contradiction']\n",
      "9\n",
      "words: ['israel' 'arab' 'palestinians' 'israeli' 'arabs' 'israelis' 'palestinian'\n",
      " 'palestine' 'jewish' 'jews' 'occupied' 'territories' 'gaza' 'lebanon'\n",
      " 'jerusalem' 'peace' 'zionist' 'country' 'jew' 'occupation' 'syria'\n",
      " 'lebanese' 'territory' 'land' 'zionism' 'civilians' 'borders' 'jordan'\n",
      " 'negotiations' 'rights' 'inhabitants' 'propaganda' 'countries' 'killed'\n",
      " 'political' 'killing' 'attacking' 'state' 'attacks' 'actions' 'soldiers'\n",
      " 'war' 'who' 'minister' 'villages' 'blame' 'egypt' 'racist' 'lives'\n",
      " 'discrimination']\n"
     ]
    }
   ],
   "source": [
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:14048, Score:0.6952275037765503\n",
      "------------------\n",
      "Well my last two motorcycles have been shaft driven and they will wheelie.\n",
      "The rear gear does climb the ring gear and lift the rear which gives an\n",
      "odd feel, but it still wheelies.\n",
      "\n",
      "------------------\n",
      "\n",
      "Document:16034, Score:0.6860707998275757\n",
      "------------------\n",
      "\n",
      "\tNot to start *another* shaft effect discussion, but the twist you\n",
      "\tfeel when revving sitting still is due to the larger fly-wheels that\n",
      "\tthe BMW R-bikes (maybe K's too, dunno) use. If you whack the throttle\n",
      "\tat stop lights, it'll really rock the bike over (to the right).\n",
      "\n",
      "\t<snip> \t<snip>\n",
      "\n",
      "\tPlease post if you come to any conclusion on this. I am thinking\n",
      "\tof putting a light on each cylinder guard on my R100S, and was\n",
      "\twondering whether I was going to have to switch lights off every\n",
      "\ttime I was under 5,000 RPM :-)\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "Document:8981, Score:0.6725623607635498\n",
      "------------------\n",
      "]Is it possible to do a \"wheelie\" on a motorcycle with shaft-drive?\n",
      "\n",
      "yes.\n",
      "\n",
      "------------------\n",
      "\n",
      "Document:519, Score:0.6501535773277283\n",
      "------------------\n",
      "\n",
      "I'll throw in a vote for a Metzler \"economy\" tire, the ME77. Good\n",
      "for mid-size older bikes. Rated to 130mph. Wearing well and handles\n",
      "my 12 mile ride(twisties) to work well on the SR500. Costs a bit \n",
      "more than the Chengs/IRC's etc, but still less than the Sport\n",
      "Metzlers for the newer bikes. Cost from Chaparral is about $60 for the\n",
      "front, and $70 for the rear.\n",
      "------------------\n",
      "\n",
      "Document:5984, Score:0.6491450071334839\n",
      "------------------\n",
      "Do any Honda gurus know if I can replace the \n",
      "the front sprocket on my 1979 Honda CB750K with a slightly larger one?\n",
      "(I see this as being preferable to reducing the size of the rear one)\n",
      "\n",
      "Just wanting ride at a more relaxed RPM.\n",
      "------------------\n",
      "\n",
      "Document:18684, Score:0.6270655393600464\n",
      "------------------\n",
      "3500 miles, black leather tank bra, tank bag, Corbin seat, Metzler 'B'\n",
      "tires.  Garaged and pampered.  I can't afford to continue paying NYC garage\n",
      "fees for two bikes so one of 'em has to go.\n",
      "\n",
      "Best offer above $4500 takes it.\n",
      "\n",
      "------------------\n",
      "\n",
      "Document:10685, Score:0.6248645186424255\n",
      "------------------\n",
      "\n",
      "Well, you really can't dig a hole with a stock Shovel; you at least need some\n",
      "performance mods like stroking and cams.  Besides, it's REAL bad on the\n",
      "rear tire.\n",
      "------------------\n",
      "\n",
      "Document:5109, Score:0.6239303350448608\n",
      "------------------\n",
      "\n",
      "...\n",
      "\n",
      "Speedy, you've got this all wrong.  When you're done, buy a better dirt\n",
      "bike, body armor, decent boots, and forget about the weenie street riding.\n",
      ":-) \n",
      "------------------\n",
      "\n",
      "Document:4110, Score:0.6225978136062622\n",
      "------------------\n",
      "PLEASE DO NOT RESPOND DIRECTLY TO THIS ACCOUNT\n",
      "\n",
      "FOR SALE:\n",
      "\n",
      "Blue 1984 Toyota pickup truck with white blazer topper, AM/FM/Casette,\n",
      "A/C, cruise control.  Great for camping trips.\n",
      "\n",
      "New: brakes, master brake cylinder, Michelin tires, shocks,\n",
      "maintenance free battery, clutch, windshield wipers.\n",
      "\n",
      "Well maintained with all Toyota parts (all repairs done at the\n",
      "dealers.)  Very little rust, body in good shape.  \n",
      "\n",
      "126K miles\n",
      "\n",
      "Asking $2800.  If interested, please contact:\n",
      "\n",
      "   Ursula Fritsch\n",
      "   umf@gene.com\n",
      "   (415)-347-6813\n",
      "\n",
      "\t    PLEASE DO NOT RESPOND DIRECTLY TO THIS ACCOUNT\n",
      "\n",
      "------------------\n",
      "\n",
      "Document:1503, Score:0.6116603016853333\n",
      "------------------\n",
      "\n",
      "\n",
      "\n",
      "Shades of the Edsel! They had pushbuttons in the steering wheel hub\n",
      "that controlled the auto tranny. It was very disconcerting to shift\n",
      "into reverse when turning a corner and the wires shorted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=0, num_docs=10)\n",
    "\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f'Document:{doc_id}, Score:{score}')\n",
    "    print('------------------')\n",
    "    print(doc)\n",
    "    print('------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_cluster_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 23:57:14,609 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-14 23:57:19,848 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 00:01:09,550 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 00:01:16,022 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 00:01:16,894 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':50, 'min_samples':None, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 1\n",
      "\n",
      "\n",
      "0\n",
      "words: ['but' 'anyone' 'some' 'quite' 'any' 'very' 'this' 'sort' 'probably'\n",
      " 'someone' 'few' 'idea' 'really' 'guess' 'be' 'please' 'think' 'don'\n",
      " 'pretty' 'ideas' 'more' 'enough' 'might' 'to' 'thanks' 'if' 'rather' 'am'\n",
      " 'sure' 'would' 'get' 'make' 'making' 'you' 'something' 'your' 'things'\n",
      " 'point' 'bike' 'just' 'that' 'like' 'doing' 'little' 'good' 'lot' 'real'\n",
      " 'interesting' 'one' 'makes']\n",
      "1\n",
      "words: ['shameful' 'dsl' 'jxp' 'geb' 'chastity' 'intellect' 'skepticism' 'cadre'\n",
      " 'pitt' 'gordon' 'surrender' 'banks' 'soon' 'too' 'patients' 'lyme'\n",
      " 'effective' 'patient' 'it' 'rare' 'sometimes' 'heart' 'treatment'\n",
      " 'physician' 'much' 'is' 'weight' 'practical' 'disease' 'drugs'\n",
      " 'difficult' 'medicine' 'physicians' 'doctor' 'hurt' 'pressure' 'hope'\n",
      " 'blood' 'mean' 'gant' 'isn' 'safe' 'wouldn' 'afraid' 'breath' 'gps'\n",
      " 'dose' 'citizen' 'especially' 'walks']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_cluster_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 01:06:54,332 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-15 01:06:59,746 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 01:10:43,272 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 01:10:49,766 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 01:10:50,618 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':20, 'min_samples':None, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 85\n",
      "\n",
      "\n",
      "0\n",
      "words: ['god' 'jesus' 'christ' 'faith' 'bible' 'christians' 'scripture'\n",
      " 'christianity' 'truth' 'scriptures' 'heaven' 'belief' 'life' 'christian'\n",
      " 'holy' 'believers' 'spirit' 'sins' 'eternal' 'gospel' 'revelation'\n",
      " 'spiritual' 'luke' 'doctrines' 'love' 'salvation' 'church' 'contradict'\n",
      " 'theology' 'testament' 'himself' 'prophets' 'doctrine' 'sin' 'religions'\n",
      " 'evil' 'interpretation' 'believing' 'biblical' 'beliefs' 'lord' 'divine'\n",
      " 'nature' 'religion' 'believe' 'resurrection' 'existence' 'romans' 'psalm'\n",
      " 'worship']\n",
      "1\n",
      "words: ['bike' 'car' 'ride' 'cars' 'honda' 'brakes' 'rear' 'bikes' 'tires'\n",
      " 'engine' 'wheel' 'riding' 'mph' 'wheels' 'miles' 'gear' 'throttle'\n",
      " 'suspension' 'brake' 'motorcycle' 'toyota' 'tire' 'ford' 'passenger'\n",
      " 'driving' 'steering' 'seat' 'bmw' 'front' 'valve' 'cylinder' 'torque'\n",
      " 'shaft' 'mileage' 'pedal' 'stock' 'parking' 'abs' 'clutch' 'highway'\n",
      " 'buy' 'rpm' 'bought' 'fun' 'exhaust' 'buying' 'dealer' 'lights' 'mile'\n",
      " 'sho']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_cluster_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 01:12:04,788 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-15 01:12:10,184 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 01:15:49,133 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 01:15:55,608 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 01:15:56,489 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':25, 'min_samples':None, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 1\n",
      "\n",
      "\n",
      "0\n",
      "words: ['few' 'sort' 'guess' 'but' 'very' 'more' 'ideas' 'any' 'some' 'lot'\n",
      " 'appreciate' 'enough' 'just' 'really' 'way' 'those' 'like' 'time' 'long'\n",
      " 'it' 'probably' 'anything' 'either' 'your' 'someone' 'almost' 'idea'\n",
      " 'much' 'make' 'quite' 'having' 'where' 'anyone' 'thanks' 'little' 'folks'\n",
      " 'for' 'something' 'could' 'can' 'important' 'well' 'you' 'actually' 'get'\n",
      " 'pretty' 'here' 'even' 'serious' 'find']\n",
      "1\n",
      "words: ['jxp' 'shameful' 'skepticism' 'dsl' 'chastity' 'intellect' 'geb' 'cadre'\n",
      " 'pitt' 'surrender' 'gordon' 'banks' 'soon' 'too' 'lyme' 'patients'\n",
      " 'patient' 'afraid' 'dose' 'disease' 'effective' 'it' 'physician'\n",
      " 'physicians' 'meant' 'weight' 'treated' 'enough' 'doing' 'sometimes'\n",
      " 'medicine' 'pressure' 'few' 'hurt' 'blood' 'bread' 'edu' 'gant' 'doctor'\n",
      " 'meat' 'rare' 'emergency' 'breath' 'fault' 'citizen' 'probably' 'county'\n",
      " 'liver' 'recommendations' 'become']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_cluster_size = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 01:17:03,073 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-15 01:17:08,431 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 01:20:48,648 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 01:20:55,452 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 01:20:55,767 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':23, 'min_samples':None, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 61\n",
      "\n",
      "\n",
      "0\n",
      "words: ['god' 'jesus' 'christ' 'faith' 'christianity' 'scripture' 'bible'\n",
      " 'christians' 'belief' 'truth' 'scriptures' 'heaven' 'holy' 'gospel'\n",
      " 'believers' 'eternal' 'christian' 'spirit' 'revelation' 'church' 'life'\n",
      " 'sins' 'luke' 'theology' 'divine' 'lord' 'doctrines' 'contradict' 'sin'\n",
      " 'salvation' 'spiritual' 'believe' 'nature' 'doctrine' 'prophets'\n",
      " 'biblical' 'accept' 'testament' 'resurrection' 'corinthians' 'evil'\n",
      " 'meaning' 'matthew' 'religion' 'beliefs' 'judgement' 'notion' 'passage'\n",
      " 'sense' 'believing']\n",
      "1\n",
      "words: ['car' 'bike' 'cars' 'ride' 'tires' 'honda' 'brakes' 'bikes' 'rear'\n",
      " 'engine' 'mph' 'riding' 'wheel' 'gear' 'wheels' 'throttle' 'brake'\n",
      " 'motorcycle' 'suspension' 'tire' 'passenger' 'miles' 'steering' 'toyota'\n",
      " 'driving' 'front' 'seat' 'ford' 'torque' 'cylinder' 'valve' 'bmw'\n",
      " 'clutch' 'pedal' 'shaft' 'fun' 'abs' 'parking' 'dealer' 'mileage' 'cage'\n",
      " 'highway' 'stock' 'buy' 'oil' 'buying' 'rpm' 'lights' 'exhaust' 'tank']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_cluster_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 01:21:32,079 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-15 01:21:37,759 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 01:25:16,108 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 01:25:23,889 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 01:25:24,205 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':24, 'min_samples':None, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 67\n",
      "\n",
      "\n",
      "0\n",
      "words: ['god' 'jesus' 'faith' 'christ' 'christians' 'christianity' 'truth'\n",
      " 'bible' 'scripture' 'life' 'eternal' 'believers' 'belief' 'gospel'\n",
      " 'scriptures' 'christian' 'church' 'heaven' 'lord' 'holy' 'divine' 'evil'\n",
      " 'sins' 'believe' 'revelation' 'spiritual' 'luke' 'doctrine' 'biblical'\n",
      " 'contradict' 'nature' 'believing' 'spirit' 'accept' 'religions' 'love'\n",
      " 'theology' 'salvation' 'resurrection' 'testament' 'judgement' 'doctrines'\n",
      " 'beliefs' 'prophets' 'atheists' 'corinthians' 'religion' 'meaning'\n",
      " 'teachings' 'sin']\n",
      "1\n",
      "words: ['car' 'bike' 'cars' 'ride' 'honda' 'bikes' 'tires' 'engine' 'riding'\n",
      " 'rear' 'brakes' 'mph' 'wheel' 'wheels' 'gear' 'suspension' 'motorcycle'\n",
      " 'brake' 'tire' 'throttle' 'miles' 'bmw' 'driving' 'toyota' 'front' 'ford'\n",
      " 'passenger' 'seat' 'steering' 'cylinder' 'torque' 'valve' 'shaft'\n",
      " 'clutch' 'pedal' 'buy' 'parking' 'stock' 'highway' 'mileage' 'road' 'abs'\n",
      " 'fun' 'dealer' 'rpm' 'motorcycles' 'garage' 'exhaust' 'truck' 'owner']\n",
      "2\n",
      "words: ['mail' 'please' 'email' 'send' 'mailing' 'address' 'thanks' 'reply'\n",
      " 'information' 'interested' 'articles' 'post' 'appreciate' 'list'\n",
      " 'posting' 'addresses' 'thank' 'request' 'replies' 'newsgroup' 'via'\n",
      " 'interest' 'bitnet' 'internet' 'technical' 'listserv' 'info' 'topic'\n",
      " 'subscribe' 'org' 'helpful' 'questions' 'contact' 'welcome' 'site'\n",
      " 'student' 'article' 'subscription' 'readers' 'suggestions' 'project'\n",
      " 'com' 'fax' 'message' 'newsgroups' 'news' 'available' 'faq' 'computer'\n",
      " 'posted']\n",
      "3\n",
      "words: ['really' 'stupid' 'think' 'lot' 'anything' 'wouldn' 'happen' 'guess'\n",
      " 'say' 'people' 'don' 'didn' 'flame' 'maybe' 'oh' 'tell' 'yeah' 'just'\n",
      " 'going' 'isn' 're' 'feel' 'that' 'something' 'joke' 'you' 'serious' 'why'\n",
      " 'thing' 'll' 'take' 'thought' 'anyway' 'good' 'about' 'ask' 'even' 'ever'\n",
      " 'hey' 'do' 'everyone' 'admit' 'course' 'someone' 'bad' 'wrong' 'kids'\n",
      " 'certainly' 'shit' 'glad']\n",
      "4\n",
      "words: ['team' 'playoffs' 'game' 'scoring' 'season' 'playoff' 'nhl' 'play'\n",
      " 'teams' 'hockey' 'defenseman' 'overtime' 'winnipeg' 'flyers' 'puck'\n",
      " 'goal' 'quebec' 'scored' 'leafs' 'games' 'goalie' 'islanders' 'players'\n",
      " 'pens' 'potvin' 'calgary' 'score' 'goals' 'sabres' 'canucks' 'devils'\n",
      " 'played' 'gilmour' 'blues' 'detroit' 'winning' 'buffalo' 'playing'\n",
      " 'penguins' 'jets' 'coach' 'edmonton' 'fans' 'isles' 'espn' 'penalty'\n",
      " 'tampa' 'finals' 'player' 'cup']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(5)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'min_samples':5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 01:28:45,986 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-15 01:28:51,343 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 01:32:31,180 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 01:32:37,736 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 01:32:38,591 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':24, 'min_samples':5, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 103\n",
      "\n",
      "\n",
      "0\n",
      "words: ['bike' 'car' 'ride' 'cars' 'honda' 'brakes' 'bikes' 'engine' 'mph' 'rear'\n",
      " 'tires' 'riding' 'wheel' 'gear' 'wheels' 'motorcycle' 'brake' 'miles'\n",
      " 'suspension' 'throttle' 'front' 'tire' 'steering' 'driving' 'toyota'\n",
      " 'ford' 'passenger' 'seat' 'bmw' 'valve' 'pedal' 'cylinder' 'torque'\n",
      " 'shaft' 'abs' 'clutch' 'parking' 'mileage' 'dealer' 'stock' 'buying'\n",
      " 'cage' 'bought' 'exhaust' 'lights' 'oil' 'rpm' 'tank' 'highway'\n",
      " 'motorcycles']\n",
      "1\n",
      "words: ['god' 'jesus' 'christ' 'faith' 'christians' 'scripture' 'bible'\n",
      " 'christianity' 'heaven' 'scriptures' 'truth' 'gospel' 'eternal' 'holy'\n",
      " 'christian' 'salvation' 'revelation' 'spirit' 'luke' 'lord' 'sins'\n",
      " 'belief' 'church' 'believers' 'doctrine' 'sin' 'life' 'prophets' 'divine'\n",
      " 'theology' 'resurrection' 'himself' 'romans' 'doctrines' 'isaiah'\n",
      " 'passages' 'biblical' 'psalm' 'psalms' 'spiritual' 'testament'\n",
      " 'corinthians' 'apostles' 'beliefs' 'matthew' 'love' 'contradict' 'verses'\n",
      " 'worship' 'religions']\n",
      "2\n",
      "words: ['think' 'didn' 'don' 'really' 'just' 'stupid' 'oh' 'you' 'guess'\n",
      " 'anything' 'wouldn' 'thing' 'maybe' 'thought' 'flame' 'people' 'ever'\n",
      " 'say' 'me' 'going' 'glad' 'something' 'about' 'like' 'anyway' 'll' 'he'\n",
      " 'myself' 'take' 'know' 'got' 'little' 'here' 'but' 'yeah' 'who' 'shit'\n",
      " 'good' 'lot' 'feel' 'why' 'they' 'someone' 'him' 'because' 'believe'\n",
      " 'even' 'nothing' 'that' 'your']\n",
      "3\n",
      "words: ['pitching' 'hitter' 'pitcher' 'pitchers' 'rbi' 'inning' 'batting'\n",
      " 'pitches' 'innings' 'hit' 'pitch' 'pitched' 'hitting' 'braves' 'baseball'\n",
      " 'season' 'score' 'career' 'players' 'batter' 'game' 'teams' 'clemens'\n",
      " 'morris' 'team' 'dodgers' 'ball' 'phillies' 'jays' 'yankees' 'mets'\n",
      " 'scored' 'bonds' 'league' 'walks' 'offense' 'reds' 'gant' 'cubs' 'sox'\n",
      " 'alomar' 'year' 'rookie' 'hits' 'player' 'games' 'he' 'defensive'\n",
      " 'playing' 'winning']\n",
      "4\n",
      "words: ['secure' 'key' 'encryption' 'escrow' 'clipper' 'keys' 'encrypted' 'nsa'\n",
      " 'scheme' 'crypto' 'algorithm' 'agencies' 'wiretap' 'encrypt' 'security'\n",
      " 'government' 'decrypt' 'chip' 'enforcement' 'des' 'phones' 'cryptography'\n",
      " 'secret' 'rsa' 'classified' 'cipher' 'cryptosystem' 'denning' 'privacy'\n",
      " 'cryptographic' 'plaintext' 'private' 'communications' 'conversations'\n",
      " 'proposal' 'ciphertext' 'legal' 'wiretaps' 'public' 'trust' 'session'\n",
      " 'agency' 'traffic' 'tap' 'feds' 'cellular' 'chips' 'court' 'algorithms'\n",
      " 'random']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(5)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'min_samples':40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 01:47:14,008 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-07-15 01:47:19,185 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-07-15 01:50:59,122 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-07-15 01:51:05,829 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-07-15 01:51:06,148 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_args = dict({'min_cluster_size':24, 'min_samples':25, 'metric':'euclidean', 'cluster_selection_method': 'eom'})\n",
    "model = Top2Vec(documents = docs, hdbscan_args = hdbscan_args)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 60\n",
      "\n",
      "\n",
      "0\n",
      "words: ['god' 'jesus' 'christianity' 'christ' 'faith' 'bible' 'christians'\n",
      " 'scripture' 'scriptures' 'truth' 'eternal' 'heaven' 'belief' 'believers'\n",
      " 'christian' 'life' 'church' 'revelation' 'gospel' 'lord' 'believe'\n",
      " 'salvation' 'luke' 'prophets' 'sins' 'contradict' 'holy' 'doctrines'\n",
      " 'divine' 'beliefs' 'testament' 'doctrine' 'spirit' 'resurrection'\n",
      " 'spiritual' 'religions' 'evil' 'religion' 'theology' 'himself' 'sin'\n",
      " 'atheists' 'nature' 'biblical' 'love' 'meaning' 'corinthians' 'matthew'\n",
      " 'say' 'believing']\n",
      "1\n",
      "words: ['bike' 'car' 'ride' 'honda' 'cars' 'riding' 'brakes' 'bikes' 'tires'\n",
      " 'engine' 'mph' 'rear' 'wheel' 'gear' 'suspension' 'motorcycle' 'tire'\n",
      " 'throttle' 'brake' 'wheels' 'passenger' 'toyota' 'miles' 'ford' 'driving'\n",
      " 'steering' 'bmw' 'seat' 'torque' 'front' 'valve' 'shaft' 'cylinder'\n",
      " 'bought' 'buying' 'parking' 'abs' 'pedal' 'clutch' 'rpm' 'road' 'stock'\n",
      " 'exhaust' 'fun' 'cage' 'motorcycles' 'dealer' 'highway' 'mileage' 'truck']\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f'Number of topics: {topic_nums[-1]}'); print(); print()\n",
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f'words: {words}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp-top2vec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b0ffce9d2fc1e60f68756666b081f190f164050d9305ae3f1ae36a7bcb6b1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
